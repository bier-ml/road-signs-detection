{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:15.615004500Z",
     "start_time": "2023-11-23T14:25:15.606447200Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from typing import Tuple, Optional, Callable\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms.v2 import ToTensor, Resize, Compose, ColorJitter, RandomRotation, AugMix, GaussianBlur, \\\n",
    "    RandomEqualize, RandomHorizontalFlip, RandomVerticalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:15.798559400Z",
     "start_time": "2023-11-23T14:25:15.792977300Z"
    }
   },
   "id": "4c079831cc123642"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 split: str,\n",
    "                 transform: Optional[Callable] = None):\n",
    "        self.base_folder = root\n",
    "        self.csv_file = os.path.join(self.base_folder, 'Train.csv' if split == 'train' else 'Test.csv')\n",
    "\n",
    "        with open(self.csv_file) as csvfile:\n",
    "            samples = [(os.path.join(self.base_folder, row['Path']), int(row['ClassId']))\n",
    "                       for row in csv.DictReader(csvfile, delimiter=',', skipinitialspace=True)\n",
    "                       ]\n",
    "\n",
    "        self.samples = samples\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        path, classId = self.samples[index]\n",
    "        sample = PIL.Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, classId"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:16.040868Z",
     "start_time": "2023-11-23T14:25:16.037328300Z"
    }
   },
   "id": "75dae11229b5c1b4"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "    RandomEqualize(0.4),\n",
    "    AugMix(),\n",
    "    RandomHorizontalFlip(0.3),\n",
    "    RandomVerticalFlip(0.3),\n",
    "    GaussianBlur((3, 3)),\n",
    "    RandomRotation(30),\n",
    "\n",
    "    Resize([50, 50]),\n",
    "    ToTensor(),\n",
    "\n",
    "])\n",
    "validation_transforms = Compose([\n",
    "    Resize([50, 50]),\n",
    "    ToTensor(),\n",
    "\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:16.345086200Z",
     "start_time": "2023-11-23T14:25:16.338501400Z"
    }
   },
   "id": "ec07ce4fc7a8d6f7"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def train_test_split(dataset, train_size):\n",
    "    train_size = int(train_size * len(dataset))\n",
    "    test_size = int(len(dataset) - train_size)\n",
    "    return random_split(dataset, [train_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:16.849022100Z",
     "start_time": "2023-11-23T14:25:16.793912900Z"
    }
   },
   "id": "b3152f0e863f4be3"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size : 31367, Validation size : 7842\n"
     ]
    }
   ],
   "source": [
    "dataset = GTSRB(root='../data/gtsrb', split=\"train\")\n",
    "train_set, validation_set = train_test_split(dataset, train_size=0.8)\n",
    "print(f'training size : {len(train_set)}, Validation size : {len(validation_set)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:17.205036600Z",
     "start_time": "2023-11-23T14:25:17.070903500Z"
    }
   },
   "id": "b19306248ff91859"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "train_set.dataset.transform = train_transforms\n",
    "validation_set.dataset.transform = validation_transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:17.886119Z",
     "start_time": "2023-11-23T14:25:17.877534Z"
    }
   },
   "id": "e8d10195b7b4f452"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_set, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:19.628032100Z",
     "start_time": "2023-11-23T14:25:19.624167100Z"
    }
   },
   "id": "8cc44a160eeb8dfa"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.metrics = {}\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.l1 = nn.Linear(1024 * 4 * 4, 512)\n",
    "        self.l2 = nn.Linear(512, 128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv5(maxpool)\n",
    "        conv = self.conv6(conv)\n",
    "        batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        flatten = self.flatten(maxpool)\n",
    "\n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_metrics(self, positives, data_size, loss):\n",
    "        acc = positives / data_size\n",
    "        return loss, acc\n",
    "\n",
    "    def validation_metrics(self, validation_data, loss_function):\n",
    "        data_size = len(validation_data)\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        model = self.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (input, label) in enumerate(validation_data):\n",
    "                input, label = input.to(device), label.to(device)\n",
    "                prediction = model.forward(input)\n",
    "                loss = loss_function(prediction, label)\n",
    "                val_loss = loss.item()\n",
    "                _, predicted = torch.max(prediction, 1)\n",
    "                correct_predictions += (predicted == label).sum().item()\n",
    "                total_samples += label.size(0)\n",
    "\n",
    "        val_acc = correct_predictions / total_samples\n",
    "\n",
    "        return val_loss, val_acc\n",
    "\n",
    "    def history(self):\n",
    "        return self.metrics\n",
    "\n",
    "    def compile(self, train_data, validation_data, epochs, loss_function, optimizer, learning_rate_scheduler):\n",
    "        val_acc_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "\n",
    "        learning_rate_list = []\n",
    "\n",
    "        print('training started ...')\n",
    "        STEPS = len(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            learning_rate_list.append(lr)\n",
    "            correct_predictions = 0\n",
    "            total_examples = 0\n",
    "            loss = 0\n",
    "            with tqdm.trange(STEPS) as progress:\n",
    "\n",
    "                for step, (input, label) in enumerate(train_loader):\n",
    "                    input, label = input.to(device), label.to(device)\n",
    "                    prediction = self.forward(input)\n",
    "\n",
    "                    _, predicted = torch.max(prediction, 1)\n",
    "                    correct_predictions += (predicted == label).sum().item()\n",
    "                    total_examples += label.size(0)\n",
    "                    l = loss_function(prediction, label)\n",
    "                    loss = l.item()\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    progress.colour = 'green'\n",
    "                    progress.desc = f'Epoch [{epoch}/{EPOCHS}], Step [{step}/{STEPS}], Learning Rate [{lr}], Loss [{\"{:.4f}\".format(l)}], Accuracy [{\"{:.4f}\".format(correct_predictions / total_examples)}]'\n",
    "                    progress.update(1)\n",
    "\n",
    "            training_loss, training_acc = self.training_metrics(correct_predictions, total_examples, loss)\n",
    "            train_acc_list.append(training_acc)\n",
    "            train_loss_list.append(training_loss)\n",
    "\n",
    "            val_loss, val_acc = self.validation_metrics(validation_data, loss_function)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "            print(f'val_accuracy [{val_acc}], val_loss [{val_loss}]')\n",
    "\n",
    "            learning_rate_scheduler.step()\n",
    "\n",
    "        metrics_dict = {\n",
    "            'train_acc': train_acc_list,\n",
    "            'train_loss': train_loss_list,\n",
    "            'val_acc': val_acc_list,\n",
    "            'val_loss': val_loss_list,\n",
    "            'learning_rate': optimizer.param_groups[0][\"lr\"]\n",
    "        }\n",
    "        self.metrics = metrics_dict\n",
    "        print('training complete !')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:25:20.937666700Z",
     "start_time": "2023-11-23T14:25:20.884260100Z"
    }
   },
   "id": "b660b8fe61b65d20"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "INPUT_DIM = 3 * 50 * 50\n",
    "OUTPUT_DIM = 43\n",
    "model = CNN(INPUT_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "lr_s = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=10)\n",
    "loss = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:27:53.573871500Z",
     "start_time": "2023-11-23T14:27:52.985096900Z"
    }
   },
   "id": "31c0822c97f6041f"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Step [490/491], Learning Rate [0.001], Loss [0.0263], Accuracy [0.5776]: 100%|\u001B[32m██████████\u001B[0m| 491/491 [02:47<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9517980107115531], val_loss [0.2991262972354889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [490/491], Learning Rate [0.00095], Loss [0.0150], Accuracy [0.9807]: 100%|\u001B[32m██████████\u001B[0m| 491/491 [02:49<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9908186687069626], val_loss [0.07485243678092957]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [490/491], Learning Rate [0.0009000000000000001], Loss [0.0154], Accuracy [0.9953]: 100%|\u001B[32m██████████\u001B[0m| 491/491 [02:47<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9914562611578679], val_loss [0.08432421088218689]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [490/491], Learning Rate [0.0008500000000000001], Loss [0.0009], Accuracy [0.9989]: 100%|\u001B[32m██████████\u001B[0m| 491/491 [02:25<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9978321856669217], val_loss [0.0047309850342571735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [490/491], Learning Rate [0.0008], Loss [0.0004], Accuracy [0.9999]: 100%|\u001B[32m██████████\u001B[0m| 491/491 [02:41<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9980872226472839], val_loss [0.0041684540919959545]\n",
      "training complete !\n"
     ]
    }
   ],
   "source": [
    "model.compile(train_data=train_loader, validation_data=validation_loader, epochs=EPOCHS, loss_function=loss,\n",
    "              optimizer=optimizer, learning_rate_scheduler=lr_s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:44:39.103231500Z",
     "start_time": "2023-11-23T14:27:53.576671300Z"
    }
   },
   "id": "3f084d35d8c55ca"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/best_CNN.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:47:14.109914400Z",
     "start_time": "2023-11-23T14:47:13.142850100Z"
    }
   },
   "id": "7e0a1d2e541be14f"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "model2 = torch.load('../models/best_CNN.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:47:30.141463100Z",
     "start_time": "2023-11-23T14:47:29.997284300Z"
    }
   },
   "id": "df6aa436b8801fc3"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# define a transform to convert the image to tensor and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),  # resize the image to the same size as your trained model input size\n",
    "    transforms.ToTensor(),  # convert the image to a PyTorch Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalize the image (optional)\n",
    "])\n",
    "\n",
    "url = 'https://i.ytimg.com/vi/D-rXSzu_fLk/maxresdefault.jpg'\n",
    "r = requests.get(url, stream=True)\n",
    "img = Image.open(io.BytesIO(r.content))\n",
    "\n",
    "input = transform(img)\n",
    "input = input.unsqueeze(0).to(device)\n",
    "output = model(input)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "print(probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:00:26.531079200Z",
     "start_time": "2023-11-23T15:00:26.325516200Z"
    }
   },
   "id": "54239b47a8a7ef55"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(9.2053, device='cuda:0', grad_fn=<MaxBackward1>)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:00:57.766611200Z",
     "start_time": "2023-11-23T15:00:57.752580700Z"
    }
   },
   "id": "8709e273482ba347"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "_, top_class = probabilities.max(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:01:36.547639700Z",
     "start_time": "2023-11-23T15:01:36.537928800Z"
    }
   },
   "id": "9da834bc8b53623e"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(12, device='cuda:0')"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:01:39.671099500Z",
     "start_time": "2023-11-23T15:01:39.648500800Z"
    }
   },
   "id": "e0b2754908f0f19f"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "top_prob, top_classes = probabilities.topk(k=3, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:01:48.070716400Z",
     "start_time": "2023-11-23T15:01:47.399199400Z"
    }
   },
   "id": "eeb7ba6396a7c464"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.7102, 0.2098, 0.0251], device='cuda:0', grad_fn=<TopkBackward0>)"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T15:01:49.985149200Z",
     "start_time": "2023-11-23T15:01:49.958951700Z"
    }
   },
   "id": "326308a7ae692b9"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([12, 17, 25], device='cuda:0')"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30003e5ed7b0b9a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6bbea67110cd1e4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
